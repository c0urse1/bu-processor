# ğŸ¤– BU-Processor: AI-Powered Document Analysis Pipeline

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

Ein hochmoderner, KI-gestÃ¼tzter Pipeline fÃ¼r die automatische Analyse und Klassifikation von deutschen BerufsunfÃ¤higkeits-Dokumenten mit semantischem Chunking, Vektor-Datenbank-Integration und intelligenter Deduplication.

## ğŸš€ Features

### ğŸ§  **Intelligente Dokumentanalyse**
- **Multi-Engine PDF-Extraktion** mit PyMuPDF, PyPDF2 und Fallback-Mechanismen
- **Semantisches Chunking** mit SentenceTransformers fÃ¼r bessere Segmentierung
- **Hierarchische Dokumentstruktur-Erkennung** (Ãœberschriften, Inhaltsverzeichnisse)
- **Adaptive Chunk-GrÃ¶ÃŸen** basierend auf Content-Type

### ğŸ” **Machine Learning Pipeline**
- **BERT-basierte Klassifikation** fÃ¼r BU-Dokument-Kategorien
- **Batch-Verarbeitung** fÃ¼r hochperformante Analyse
- **Confidence-basierte QualitÃ¤tskontrolle** mit konfigurierbaren Schwellwerten
- **GPU-Acceleration** fÃ¼r schnellere Inferenz

### ğŸŒŠ **Erweiterte Datenverarbeitung**
- **SimHash-basierte Deduplication** zur Erkennung semantischer Duplikate
- **Pinecone Vector Database Integration** fÃ¼r Ã„hnlichkeitssuche
- **Semantic Clustering** mit DBSCAN und Agglomerative Clustering
- **Multi-linguale Embeddings** (DE/EN) mit optimiertem Caching

### ğŸ› ï¸ **Enterprise-Ready Architecture**
- **Pydantic-basierte Konfiguration** mit Environment-Management
- **Strukturiertes Logging** mit Retry-Mechanismen und Error-Handling
- **RESTful API** mit FastAPI und automatischer Dokumentation
- **Docker-Support** fÃ¼r einfache Deployment

### ğŸ¯ **Spezialisierung fÃ¼r BU-Dokumente**
- **Deutsche Rechtstexte** mit Legal-Text-Optimierung
- **Versicherungsprodukt-Klassifikation** 
- **Compliance-konforme Verarbeitung** sensibler Daten
- **Automatische QualitÃ¤tsbewertung** fÃ¼r Dokumentinhalte

## ğŸ“‹ Anforderungen

- **Python 3.8+**
- **4GB+ RAM** (8GB empfohlen fÃ¼r grÃ¶ÃŸere Dokumente)
- **CUDA-kompatible GPU** (optional, fÃ¼r bessere Performance)

## âš¡ Quick Start

### ğŸ“¦ Installation
```bash
# One-liner Installation
pip install . && cp .env.example .env && echo "âœ… Ready to go!"

# Optional: Edit configuration
nano .env  # Linux/Mac
# notepad .env  # Windows
```

### ğŸš€ Usage (2 Befehle)
```bash
# 1. Einzelnes PDF analysieren
python -m bu_processor.pipeline --input data/sample.pdf --output out/

# 2. Kompletten Ordner verarbeiten
python -m bu_processor.pipeline --input data/ --output out/ --batch
```

### ğŸ’» CLI Power User
```bash
# Semantic Chunking + Klassifikation
python -m bu_processor.cli classify document.pdf --strategy semantic --confidence 0.8

# REST API Server starten
python -m bu_processor.cli api --host 0.0.0.0 --port 8000 --reload
# ğŸŒ Browser: http://localhost:8000/docs

# Text direkt klassifizieren
python -m bu_processor.cli classify "Ich arbeite als Softwareentwickler" --format json

# Alle verfÃ¼gbaren Befehle
python -m bu_processor.cli --help
```

### ğŸ Python API (Minimal)
```python
from bu_processor.pipeline.classifier import RealMLClassifier

# One-Shot Classification
classifier = RealMLClassifier()
result = classifier.classify_pdf("document.pdf")

print(f"ğŸ“„ {result.category} ({result.confidence:.0%})")
print(f"ğŸ“Š {len(result.chunks)} chunks extracted")
```

### ğŸ Python API (Erweitert)
```python
from bu_processor.pipeline.classifier import RealMLClassifier
from bu_processor.pipeline.pdf_extractor import ChunkingStrategy, ContentType
from bu_processor.core.config import get_config

# Konfiguration laden
config = get_config()
print(f"ğŸ—ï¸ Environment: {config.environment.value}")

# Classifier mit Custom Settings
classifier = RealMLClassifier(
    confidence_threshold=0.9,
    use_gpu=True
)

# Fortgeschrittene Klassifikation
result = classifier.classify_pdf(
    "complex_document.pdf",
    chunking_strategy=ChunkingStrategy.SEMANTIC,
    content_type=ContentType.LEGAL_TEXT,
    extract_metadata=True
)

# Detaillierte Ergebnisse
print(f"ğŸ“‹ Kategorie: {result.category}")
print(f"ğŸ¯ Confidence: {result.confidence:.3f}")
print(f"ğŸ“„ Seiten: {result.metadata.get('page_count', 'unbekannt')}")
print(f"ğŸ§© Chunks: {len(result.chunks)}")

# Chunk-Details
for i, chunk in enumerate(result.chunks[:3]):
    print(f"  {i+1}. {chunk.content_type.value}: {chunk.text[:100]}...")
```

### ğŸ”§ Development Setup
```bash
# Komplettes Development Environment
git clone https://github.com/yourusername/bu-processor.git
cd bu-processor
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\\Scripts\\activate   # Windows

pip install -r requirements-dev.txt
pre-commit install
cp .env.example .env

# Teste Installation
python cli.py config  # Zeigt aktuelle Konfiguration
python -m pytest tests/ -v  # Laufe Tests
```

### âš™ï¸ Environment Konfiguration (.env)
```bash
# Basis-Konfiguration (development)
BU_PROCESSOR_ENVIRONMENT=development
BU_PROCESSOR_ML_MODEL__MODEL_PATH=bert-base-german-cased
BU_PROCESSOR_PDF_PROCESSING__MAX_PDF_SIZE_MB=50
BU_PROCESSOR_VECTOR_DB__ENABLE_VECTOR_DB=false

# Optional: Erweiterte Features
# PINECONE_API_KEY=your-pinecone-key  # Vector Database
# OPENAI_API_KEY=your-openai-key      # Chatbot
# BU_PROCESSOR_API__SECRET_KEY=secure-secret-key
```

## ğŸ—ï¸ Architektur

```
bu-processor/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â””â”€â”€ config.py          # Zentrale Konfiguration
â”‚   â”œâ”€â”€ pipeline/
â”‚   â”‚   â”œâ”€â”€ pdf_extractor.py   # PDF-Verarbeitung
â”‚   â”‚   â”œâ”€â”€ classifier.py      # ML-Klassifikation
â”‚   â”‚   â”œâ”€â”€ semantic_chunking_enhancement.py
â”‚   â”‚   â””â”€â”€ simhash_semantic_deduplication.py
â”‚   â”œâ”€â”€ api/                   # REST API
â”‚   â”œâ”€â”€ training/              # Model Training
â”‚   â””â”€â”€ evaluation/            # Performance Metrics
â”œâ”€â”€ tests/                     # Unit & Integration Tests
â”œâ”€â”€ scripts/                   # Utility Scripts
â””â”€â”€ utils/                     # Helper Functions
```

## ğŸ”§ Konfiguration

Die Anwendung nutzt **Pydantic BaseSettings** fÃ¼r typisierte, validierte Konfiguration:

```python
# Environment-Variablen in .env
BU_PROCESSOR_ENVIRONMENT=development
BU_PROCESSOR_ML_MODEL__MODEL_PATH=bert-base-german-cased
BU_PROCESSOR_PDF_PROCESSING__MAX_PDF_SIZE_MB=50
BU_PROCESSOR_VECTOR_DB__ENABLE_VECTOR_DB=false
```

### ğŸŒ Environment Management
- **Development**: Maximale Features, Debug-Logging, alle Validierungen
- **Staging**: Production-nah mit umfassendem Logging, Performance-Tests
- **Production**: Optimiert fÃ¼r Performance, Security-hardened, minimales Logging

```bash
# Environment wechseln
export BU_PROCESSOR_ENVIRONMENT=production
# oder in .env: BU_PROCESSOR_ENVIRONMENT=production

# Config Ã¼berprÃ¼fen
python cli.py config
```

## ğŸ“Š Performance

### Benchmarks (auf Intel i7, 16GB RAM)
- **PDF-Extraktion**: ~2-5 Sekunden pro Dokument (10-50 Seiten)
- **Klassifikation**: ~100-500ms pro Dokument
- **Batch-Verarbeitung**: ~10-50 Dokumente/Minute
- **Semantic Chunking**: ~5-15 Chunks pro Dokument

### Optimierungen
- âœ… **Batch-Verarbeitung** fÃ¼r besseren Durchsatz
- âœ… **Embedding-Caching** reduziert Rechenzeit um 60-80%
- âœ… **GPU-Acceleration** fÃ¼r 3-5x schnellere Inferenz
- âœ… **Parallel PDF-Processing** fÃ¼r groÃŸe Dokumente

## ğŸ§ª Erweiterte Features

### Semantic Chunking
```python
from src.pipeline.semantic_chunking_enhancement import SemanticClusteringEnhancer

enhancer = SemanticClusteringEnhancer()
result = enhancer.enhance_chunks_with_semantic_clustering(
    chunks=document_chunks,
    content_type=ContentType.LEGAL_TEXT,
    use_hierarchical_context=True
)
```

### Deduplication
```python
from src.pipeline.simhash_semantic_deduplication import SemanticDeduplicator

deduplicator = SemanticDeduplicator()
unique_chunks = deduplicator.deduplicate_chunks_semantic(
    chunks=all_chunks,
    similarity_threshold=0.85
)
```

### Vector Database Integration
```python
# Pinecone Integration aktivieren
BU_PROCESSOR_VECTOR_DB__ENABLE_VECTOR_DB=true
BU_PROCESSOR_VECTOR_DB__PINECONE_API_KEY=your-api-key

# Automatische Ã„hnlichkeitssuche
similar_docs = classifier.find_similar_documents(
    query_text=\"BerufsunfÃ¤higkeitsversicherung\",
    top_k=5
)
```

## ğŸ§ª Testing

```bash
# Unit Tests
python -m pytest tests/ -v

# Coverage Report
python -m pytest tests/ --cov=src --cov-report=html

# Performance Tests
python -m pytest tests/performance/ -v

# Integration Tests
python -m pytest tests/integration/ -v

# API Tests
python test_api.py
```

## ğŸ“š REST API Server

### ğŸš€ API Server starten
```bash
# Option 1: CLI Command
python -m bu_processor.cli api --host 0.0.0.0 --port 8000 --reload

# Option 2: Direct Script
python start_api.py --host 0.0.0.0 --port 8000 --reload

# Option 3: Docker
docker-compose up -d
```

### ğŸ“š Interaktive Dokumentation
- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc
- **Health Check**: http://localhost:8000/health

### ğŸ”§ Beispiel API-Calls
```bash
# Text klassifizieren
curl -X POST "http://localhost:8000/classify/text" \\
  -H "Content-Type: application/json" \\
  -d '{"text": "Ich arbeite als Softwareentwickler"}'

# PDF klassifizieren
curl -X POST "http://localhost:8000/classify/pdf" \\
  -F "file=@document.pdf" \\
  -F "chunking_strategy=semantic"

# Batch-Verarbeitung
curl -X POST "http://localhost:8000/classify/batch" \\
  -H "Content-Type: application/json" \\
  -d '{"texts": ["Text 1", "Text 2"]}'

# Model Information
curl "http://localhost:8000/models/info"
```

### ğŸ Python API Client
```python
import requests

# Einfacher Client
response = requests.post(
    "http://localhost:8000/classify/text",
    json={"text": "Beispieltext"}
)
result = response.json()
print(f"Kategorie: {result['category']}, Confidence: {result['confidence']}")
```

Detaillierte API-Dokumentation: [API_USAGE.md](API_USAGE.md)



## ğŸ”§ Development Setup

```bash
# Development Dependencies
pip install -r requirements-dev.txt

# Pre-commit Hooks
pre-commit install

# Code Formatting
black src/ tests/
isort src/ tests/

# Type Checking
mypy src/

# Linting
flake8 src/ tests/
```

## ğŸ³ Docker Deployment

```bash
# Build Image
docker build -t bu-processor .

# Run Container
docker run -p 8000:8000 -v $(pwd)/.env:/app/.env bu-processor

# Docker Compose
docker-compose up -d
```

## ğŸ“ˆ Monitoring & Observability

- **Structured Logging** mit Structlog
- **Prometheus Metrics** auf Port 9100
- **Health Checks** Ã¼ber `/health` endpoint
- **Performance Tracking** fÃ¼r alle Pipeline-Komponenten

## ğŸ¤ Contributing

**Wir freuen uns Ã¼ber Contributions!** Lies unser detailliertes [CONTRIBUTING.md](CONTRIBUTING.md) fÃ¼r alle Details.

### ğŸš€ Quick Contributor Setup
```bash
# 1. Fork & Clone
git clone https://github.com/YOURUSERNAME/bu-processor.git
cd bu-processor

# 2. Development Environment
pip install -r requirements-dev.txt
pre-commit install

# 3. Feature Branch
git checkout -b feature/amazing-feature

# 4. Code, Test, Commit
black src/ tests/ && pytest tests/ -v
git commit -m "feat: add amazing feature"
git push origin feature/amazing-feature
```

### ğŸ“‹ Contribution Areas
- ğŸ› **Bug Fixes**: [Good First Issues](https://github.com/yourusername/bu-processor/labels/good%20first%20issue)
- âœ¨ **Features**: API Extensions, Performance, New Extractors
- ğŸ“š **Documentation**: Examples, Guides, API Docs
- ğŸ§ª **Tests**: Coverage, Integration, Performance
- ğŸ”§ **Refactoring**: Code Quality, Architecture

### ğŸ“ Issue & PR Templates
Wir nutzen strukturierte Templates fÃ¼r bessere Zusammenarbeit:
- ğŸ› [Bug Report](.github/ISSUE_TEMPLATE/bug_report.md)
- âœ¨ [Feature Request](.github/ISSUE_TEMPLATE/feature_request.md)
- â“ [Question/Support](.github/ISSUE_TEMPLATE/question.md)
- ğŸ“ [Pull Request Template](.github/pull_request_template.md)

## ğŸ“„ License

Dieses Projekt ist unter der **MIT License** lizenziert - siehe [LICENSE](LICENSE) fÃ¼r Details.

## ğŸ™ Acknowledgments

- **Hugging Face Transformers** fÃ¼r BERT-Modelle
- **SentenceTransformers** fÃ¼r Embedding-Generierung  
- **Scikit-learn** fÃ¼r Clustering-Algorithmen
- **PyMuPDF** fÃ¼r robuste PDF-Verarbeitung
- **Pydantic** fÃ¼r typisierte Konfiguration

## ğŸ“ Support & Community

### ğŸ†˜ Hilfe bekommen
- ğŸ› **Bug Reports**: [Create Issue](https://github.com/yourusername/bu-processor/issues/new?template=bug_report.md)
- âœ¨ **Feature Requests**: [Request Feature](https://github.com/yourusername/bu-processor/issues/new?template=feature_request.md)
- â“ **Questions**: [Ask Question](https://github.com/yourusername/bu-processor/issues/new?template=question.md)
- ğŸ’¬ **Diskussionen**: [GitHub Discussions](https://github.com/yourusername/bu-processor/discussions)

### ğŸ“š Ressourcen
- ğŸ“– **Wiki**: [Comprehensive Guides](https://github.com/yourusername/bu-processor/wiki)
- ğŸ¯ **Examples**: [Code Examples](examples/)
- ğŸ”§ **API Docs**: [docs.yourdomain.com](https://docs.yourdomain.com)
- ğŸ“Š **Roadmap**: [Project Board](https://github.com/yourusername/bu-processor/projects)

### ğŸ·ï¸ Labels & Workflow
- `good first issue` - Perfekt fÃ¼r neue Contributors
- `help wanted` - Community Input erwÃ¼nscht
- `bug` - BestÃ¤tigte Bugs
- `enhancement` - Feature Requests
- `documentation` - Docs-Verbesserungen

---

**â­ Wenn dir dieses Projekt hilft, gib ihm einen Stern auf GitHub!**
